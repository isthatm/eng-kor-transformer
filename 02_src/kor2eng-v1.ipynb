{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:46.213656Z",
     "iopub.status.busy": "2025-12-28T07:34:46.213269Z",
     "iopub.status.idle": "2025-12-28T07:34:51.465853Z",
     "shell.execute_reply": "2025-12-28T07:34:51.465147Z",
     "shell.execute_reply.started": "2025-12-28T07:34:46.213631Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2025.9.18)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-3.2.0 sacrebleu-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "f57c3b5a-1c9f-420a-afbc-f5fb19176d95",
    "_uuid": "f0810394-9f15-46c5-8819-c49d8553689d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:51.467819Z",
     "iopub.status.busy": "2025-12-28T07:34:51.467528Z",
     "iopub.status.idle": "2025-12-28T07:34:55.867358Z",
     "shell.execute_reply": "2025-12-28T07:34:55.866488Z",
     "shell.execute_reply.started": "2025-12-28T07:34:51.467799Z"
    },
    "id": "ns4X-VCzabCh",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sentencepiece as spm\n",
    "import sacrebleu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4fb74309-e4fb-42d4-a6a8-71c4cb3ca148",
    "_uuid": "73dd8d02-fbaf-4f79-8d24-4af939be63a7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:55.868629Z",
     "iopub.status.busy": "2025-12-28T07:34:55.868206Z",
     "iopub.status.idle": "2025-12-28T07:34:55.874540Z",
     "shell.execute_reply": "2025-12-28T07:34:55.873727Z",
     "shell.execute_reply.started": "2025-12-28T07:34:55.868598Z"
    },
    "id": "ULh3IpHxJgXp",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Languages(Enum):\n",
    "    ENG = 0\n",
    "    KOR = 1\n",
    "    \n",
    "SRC_PAD_IDX    = None\n",
    "TRG_PAD_IDX    = None\n",
    "SRC_VOCAB_SIZE = None\n",
    "TRG_VOCAB_SIZE = None\n",
    "BATCH_SIZE     = 32\n",
    "\n",
    "# Early Stopping Criteria\n",
    "PATIENCE       = 10\n",
    "DELTA          = 0.01\n",
    "\n",
    "SAVE_DIR  = r\"/kaggle/working/\" # IMPORTANT: Change your output folder destination\n",
    "FROM_LANG = Languages.ENG       # IMPORTANT: Change your original language \n",
    "\n",
    "SENTENCE_PIECE_DATA = '/kaggle/input/korean-sentencepiece/sentencepiece' # IMPORTANT: Tokenizers & data directory\n",
    "\n",
    "# IMPORTANT: Change tokenizer's filename \n",
    "ENG_MODEL_PATH = os.path.join(SENTENCE_PIECE_DATA,'english_unigram.model') \n",
    "KOR_MODEL_PATH = os.path.join(SENTENCE_PIECE_DATA,'korean_unigram.model')\n",
    "\n",
    "# IMPORTANT: Change training/testing data filename\n",
    "train_korean  = os.path.join(SENTENCE_PIECE_DATA, 'korean.txt') \n",
    "train_english = os.path.join(SENTENCE_PIECE_DATA, 'english.txt')\n",
    "test_korean   = os.path.join(SENTENCE_PIECE_DATA, 'test_korean.txt')\n",
    "test_english  = os.path.join(SENTENCE_PIECE_DATA, 'test_english.txt')\n",
    "\n",
    "# The scheduler will make learning rate be a much smaller value\n",
    "# lr * 1 / sqrt(d_model) -> Choose 1.0 (dont scale up or down, we'll use the scheduler)\n",
    "LEARNING_RATE    = 1.0 # Learning rate (to be multiplied with Noam scheduler)\n",
    "SCHEDULER_FACTOR = 1.0 # Scaler to Noam scheduler, higher factor -> higher learning rate \n",
    "NUM_EPOCHS       = 100\n",
    "SMOOTHING_FACTOR = 0.1  \n",
    "TRAIN_MODEL_PATH = None\n",
    "L2_LAMBDA        = 1e-5 # L2 regularization weigth\n",
    "\n",
    "# MODEL ARCHITECTURE\n",
    "HIDDEN_DIM       = 512  # Embedded word dimension\n",
    "NUM_LAYERS       = 6\n",
    "NUM_HEADS        = 8\n",
    "MAX_SEQ_LEN      = 64\n",
    "DROPOUT_RATE     = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d64d0823-e340-4b35-9e23-e8bf4159e853",
    "_uuid": "3dee4460-6820-4f35-a9d4-d774a36e5bce",
    "collapsed": false,
    "id": "yZfbTawqe9Fu",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "28bf7bc4-c8e0-4fd3-9564-8649fa17e751",
    "_uuid": "6a453724-9538-4bb2-8c1b-8ec1a6cbe67b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:55.875511Z",
     "iopub.status.busy": "2025-12-28T07:34:55.875288Z",
     "iopub.status.idle": "2025-12-28T07:34:59.528419Z",
     "shell.execute_reply": "2025-12-28T07:34:59.527746Z",
     "shell.execute_reply.started": "2025-12-28T07:34:55.875492Z"
    },
    "id": "rrrBcP2we-9-",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "cf2d4985-1989-44b1-cc89-6f163eae512c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25])\n",
      "tensor([  53,  120,   49,   10, 2153, 5548,   31,   10, 2100,   18,    9,   21,\n",
      "           7,   38,   10, 5548,   47, 3357,   19,   10, 1003, 5548,    4,    3,\n",
      "           3])\n",
      "torch.Size([32, 24])\n",
      "tensor([    1,  2915,    26,     8,   348,  8020, 10644,    46,  5503,  3509,\n",
      "        10644,    11,     7,    24,   614,   790,  3212,   159,     4,     2,\n",
      "            3,     3,     3,     3])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 src_lines: list[str],\n",
    "                 tgt_lines: list[str],\n",
    "                 src_sp: spm.SentencePieceProcessor,\n",
    "                 trg_sp: spm.SentencePieceProcessor,\n",
    "                 max_len=64):\n",
    "\n",
    "       self.src_sp  = src_sp\n",
    "       self.trg_sp  = trg_sp\n",
    "       self.max_len = max_len\n",
    "       self.src = [src_sp.encode(s.strip(), out_type=int) for s in src_lines] # source tokens do not require <bos> & <eos>\n",
    "       self.trg = [self.add_controls(trg_sp.encode(t.strip(), out_type=int)) for t in tgt_lines] # Add <bos> & <eos> with the add_controls() function\n",
    "\n",
    "    def add_controls(self, ids):\n",
    "        # ids[:self.max_len - 2] -> reserve space for bos & eos, only matters when len(ids) > self.max_len\n",
    "        return [self.trg_sp.bos_id()] + ids[:self.max_len - 2] + [self.trg_sp.eos_id()]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.src[idx]),\n",
    "            'labels': torch.tensor(self.trg[idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "class MyCollate:\n",
    "  '''\n",
    "    Initially, each sentence has a different number of tokens, since there length\n",
    "    aren't the same\n",
    "    -> Use collate_fn to pad them to the same size\n",
    "\n",
    "    Sequences within the same batch must have identical length. This does not hold\n",
    "    for sequences among different batches.\n",
    "  '''\n",
    "  def __init__(self, pad_token_id):\n",
    "    self.pad_token_id = pad_token_id\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    # Work on the copies of the tensors\n",
    "    input_ids = [x['input_ids'].clone().detach() for x in batch]\n",
    "    labels    = [x['labels'].clone().detach() for x in batch]\n",
    "\n",
    "    # Pad sequences in each batch with the numeric token of <pad>\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n",
    "    labels    = pad_sequence(labels, batch_first=True, padding_value=self.pad_token_id)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels\n",
    "    }  \n",
    "\n",
    "def build_tokenizer_model(language: str):\n",
    "    CHAR_COVERAGE = 0.9995 if language == 'korean' else 1.000\n",
    "    VOCAB_SIZE    = 22000 if language == 'korean' else 13000 # 22000 words for korean 13000 otw\n",
    "\n",
    "    # Choose training file(s)\n",
    "    input_file = os.path.join(SENTENCE_PIECE_DATA, \"{}.txt\".format(language))  # or \"korean.txt, english.txt\" for multiple files\n",
    "\n",
    "    # Train the tokenizer\n",
    "    '''\n",
    "    Advantages of sentencepiece:\n",
    "    + Treats whitespace as part of the subword (denoted by '_')\n",
    "    + Retains morphological meaning (adv, adj, etc.) E.g. obviously -> [obvious, ly]\n",
    "      `model_type`:\n",
    "          + bpe: merges the same frequent pairs.\n",
    "          + unigram: chooses subwords with the highest likelihood (probability)\n",
    "      `character_coverage`: amount of characters covered by the model\n",
    "          + 1.0 cover all characters (Latin-based language) or 0.9995 (e.g. kanji -> ignore rare chars)\n",
    "      `pad_id`: Assign the numeric id to <pad> tokens (By default, unk_id, bos_id & eos_id are 0, 1 & 2 respectively)\n",
    "    '''\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=input_file,\n",
    "        model_prefix=\"{}_unigram\".format(language),\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        model_type=\"unigram\",\n",
    "        character_coverage=CHAR_COVERAGE,\n",
    "        pad_id=3\n",
    "    )\n",
    "\n",
    "def load_tokenizer_model(path_to_model: str) -> spm.SentencePieceProcessor:\n",
    "    sp = spm.SentencePieceProcessor(model_file=path_to_model)\n",
    "    return sp\n",
    "\n",
    "def get_lines(file_path: str):\n",
    "    lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return lines\n",
    "\n",
    "# If no tokenizer has been compiled, uncomment the following 2 lines\n",
    "# build_tokenizer_model('korean')\n",
    "# build_tokenizer_model('english')\n",
    "\n",
    "# Otw, load the tokenizers\n",
    "# Load tokenizers\n",
    "eng_sp = load_tokenizer_model(ENG_MODEL_PATH)\n",
    "kor_sp = load_tokenizer_model(KOR_MODEL_PATH)\n",
    "tokenizers = {\n",
    "    Languages.ENG: (eng_sp, kor_sp),\n",
    "    Languages.KOR: (kor_sp, eng_sp)\n",
    "}\n",
    "\n",
    "train_sentences = {\n",
    "    Languages.ENG: (train_english, train_korean),\n",
    "    Languages.KOR: (train_korean, train_english)\n",
    "}\n",
    "\n",
    "test_sentences = {\n",
    "    Languages.ENG: (test_english, test_korean),\n",
    "    Languages.KOR: (test_korean, test_english)\n",
    "}\n",
    "\n",
    "src_tkn, trg_tkn = tokenizers[FROM_LANG]\n",
    "train_src_data, train_trg_data = train_sentences[FROM_LANG]\n",
    "test_src_data, test_trg_data = test_sentences[FROM_LANG]\n",
    "\n",
    "# Maximum number of tokens in each sentence for this specific dataset\n",
    "# max_len_src = max(len(eng_sp.encode(s.strip(), out_type=int)) for s in get_lines(english_file)) # 48\n",
    "# max_len_tgt = max(len(kor_sp.encode(t.strip(), out_type=int)) for t in get_lines(korean_file))  # 32\n",
    "\n",
    "# Test the tokenizers\n",
    "# print(eng_sp.encode_as_pieces('this is obviously the right answer.'))\n",
    "# print(kor_sp.encode_as_pieces('이건 분명히 정답이다'))\n",
    "\n",
    "SRC_VOCAB_SIZE = src_tkn.get_piece_size()\n",
    "TRG_VOCAB_SIZE = trg_tkn.get_piece_size()\n",
    "\n",
    "# Get the source padding index\n",
    "SRC_PAD_IDX = src_tkn.pad_id()\n",
    "TRG_PAD_IDX = trg_tkn.pad_id()\n",
    "\n",
    "# Modulate data -> DataLoader\n",
    "train_dataset = TranslationDataset(\n",
    "    get_lines(train_src_data),\n",
    "    get_lines(train_trg_data),\n",
    "    src_tkn,\n",
    "    trg_tkn\n",
    ")\n",
    "val_dataset = TranslationDataset(\n",
    "    get_lines(test_src_data),\n",
    "    get_lines(test_trg_data),\n",
    "    src_tkn,\n",
    "    trg_tkn\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE, # Number of sequences per batch\n",
    "    shuffle=True,\n",
    "    collate_fn=MyCollate(pad_token_id=src_tkn.pad_id())\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE, # Number of sequences per batch\n",
    "    shuffle=True,\n",
    "    collate_fn=MyCollate(pad_token_id=src_tkn.pad_id())\n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch['input_ids'].shape) # e.g. torch.Size([32, max_seq_len])\n",
    "    print(batch['input_ids'][0])    # first sample's input_ids (padded)\n",
    "    print(batch['labels'].shape)    # e.g. torch.Size([32, max_seq_len])\n",
    "    print(batch['labels'][0])       # first sample's labels (padded)\n",
    "    break                           # stop after one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cbb3bbaf-b49f-4892-89b1-7a7a32202905",
    "_uuid": "bb49a77d-a283-4663-87b9-b1c5f240ba62",
    "collapsed": false,
    "id": "Ah-khgiQX7QO",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "eca76236-390a-499c-b084-35b20121cd33",
    "_uuid": "dbb1ea52-b43d-49e8-91e6-a03b94400d5b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.529526Z",
     "iopub.status.busy": "2025-12-28T07:34:59.529251Z",
     "iopub.status.idle": "2025-12-28T07:34:59.537310Z",
     "shell.execute_reply": "2025-12-28T07:34:59.536706Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.529506Z"
    },
    "id": "LWLJPcQsX8N9",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_pad_mask(input: torch.Tensor) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  input: [batch_size, seq_length]\n",
    "  \"\"\"\n",
    "  # True values = <pad> tokens -> masked\n",
    "  pad_mask = (input == SRC_PAD_IDX).unsqueeze(1).unsqueeze(1) # [B, 1, 1, L]\n",
    "  return pad_mask\n",
    "\n",
    "def create_causal_mask(seq_len, device) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  input: [batch_size, seq_length]\n",
    "\n",
    "  E.g. If seq_length == 3\n",
    "\n",
    "  [[[[False, True, True],\n",
    "     [False,  False, True],\n",
    "     [False,  False,  False]]]]\n",
    "  \"\"\"\n",
    "  causal_mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device), diagonal=1)  # shape = [L, L]\n",
    "  causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # shape = [1, 1, L, L]\n",
    "\n",
    "  return causal_mask\n",
    "\n",
    "def create_decoder_mask(src: torch.Tensor, trg) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  src: [batch_size, seq_length]\n",
    "  \"\"\"\n",
    "  device_type = src.device\n",
    "  src_pad_mask = create_pad_mask(src) # [B, 1, 1, L]\n",
    "\n",
    "  trg_seq_len = trg.size(1)\n",
    "  trg_pad_mask = create_pad_mask(trg) # [B, 1, 1, L]\n",
    "  trg_causal_mask  = create_causal_mask(trg_seq_len, device_type) # [1, 1, L, L]\n",
    "\n",
    "  return src_pad_mask, trg_pad_mask, trg_causal_mask\n",
    "\n",
    "class Scheduler(torch.optim.lr_scheduler.LambdaLR):\n",
    "  def __init__(self, optimizer, d_model, warmup_steps=4000, scale_factor=1.0):\n",
    "      self.d_model = d_model\n",
    "      self.warmup_steps = warmup_steps\n",
    "      self.scale_factor = scale_factor # Speed up learning rate\n",
    "      super().__init__(optimizer, self.lr_lambda)\n",
    "\n",
    "  def lr_lambda(self, step):\n",
    "      if step == 0:\n",
    "          step = 1\n",
    "      return self.scale_factor * (self.d_model ** -0.5) * min(step ** -0.5, step * self.warmup_steps ** -1.5) # Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af1b5cd0-bdcd-4c15-8990-295a65205c94",
    "_uuid": "1043c992-01d5-4681-9f45-23240e3aad07",
    "collapsed": false,
    "id": "alA0J0IAchI5",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "8f63a3af-4833-472d-8539-ad9cd14b0159",
    "_uuid": "bc49778b-d8c5-454b-bf8d-e8beb8b830c2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.538415Z",
     "iopub.status.busy": "2025-12-28T07:34:59.538222Z",
     "iopub.status.idle": "2025-12-28T07:34:59.564646Z",
     "shell.execute_reply": "2025-12-28T07:34:59.563824Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.538399Z"
    },
    "id": "jd9LZDN9cll8",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "      \"\"\"\n",
    "        vocab_size: the size of the vocabulary\n",
    "        d_model: the dimension of the embedding vector for EACH token\n",
    "      \"\"\"\n",
    "      super().__init__()\n",
    "      self.embed = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "      return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "91b78aa0-d4a5-4ea1-97b2-6b468d18e23a",
    "_uuid": "68350422-a588-4867-a540-6768c3975fb5",
    "collapsed": false,
    "id": "EYJcjuivbvCI",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "4d140f06-ee8d-4249-8685-030e79bd601d",
    "_uuid": "2a370b17-6e02-43a6-b975-35267187584a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.567622Z",
     "iopub.status.busy": "2025-12-28T07:34:59.567325Z",
     "iopub.status.idle": "2025-12-28T07:34:59.579901Z",
     "shell.execute_reply": "2025-12-28T07:34:59.579107Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.567601Z"
    },
    "id": "8nFRsK8Gbzwq",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=64):\n",
    "      \"\"\"\n",
    "        d_model: the dimension of the embedding vector for EACH token\n",
    "      \"\"\"\n",
    "      super().__init__()\n",
    "\n",
    "      # Odd: PE(pos, i) = sin(pos * (1 / 10000 ^ (i / d_model))) = sin(pos * div_term)\n",
    "      position = torch.arange(max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "      div_term = torch.exp(\n",
    "          torch.arange(0, d_model, 2).float() * (-math.log(10000) / d_model) # torch.arange(0, d_model, 2) generates even i's (2i)\n",
    "          ) # torch.exp() -> e^x; math.log() -> ln\n",
    "\n",
    "      pe          = torch.zeros(max_seq_len, d_model) # [max_seq_len, d_model]\n",
    "      pe[:, 0::2] = torch.sin(position * div_term) # Apply sine function to even positions (0, 2, 4, ...)\n",
    "      pe[:, 1::2] = torch.cos(position * div_term) # Apply cosine function to odd positions (1, 3, 5, ...)\n",
    "      pe          = pe.unsqueeze(0) # [1, max_seq_len, d_model] -> broadcast to batch_size later on\n",
    "\n",
    "      assert pe.shape == (1, max_seq_len, d_model), f\"[PositionalEncoding.__init__()] Expected shape (1, {max_seq_len}, {d_model}), got {self.pe.shape}\"\n",
    "      # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "      # Used for tensors that need to be on the same device as the module.\n",
    "      # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "      self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "      # Add positional encodings to token embeddings\n",
    "      # x: [batch_size, seq_len, d_model]\n",
    "      seq_len = x.size(1)\n",
    "      x = x + self.pe[:, :seq_len, :].to(x.device) # Take the first seq_len positions from the positional encoder\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "731b7fb5-2dab-4f9f-be99-61c1a420b481",
    "_uuid": "9d40af77-e3d8-4d5a-a2be-e66b16eec0b9",
    "collapsed": false,
    "id": "Fb6XcFk5HQhF",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "8dd75a5e-c34b-49f3-aff9-03d59227fac4",
    "_uuid": "8d653eb1-61e5-4f61-9c8c-1e7f4e43709c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.581137Z",
     "iopub.status.busy": "2025-12-28T07:34:59.580767Z",
     "iopub.status.idle": "2025-12-28T07:34:59.595438Z",
     "shell.execute_reply": "2025-12-28T07:34:59.594743Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.581101Z"
    },
    "id": "mnaVANa6HQLp",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a2191f15-ce40-4ea8-bd84-6e16a3433c54",
    "_uuid": "1cd1286d-3639-4acc-8d24-764a60210cd4",
    "collapsed": false,
    "id": "PBDzklZ5l9nX",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "a6747310-66e6-42c1-85b1-a593a10155b8",
    "_uuid": "0b195907-0d64-4c30-a8a0-8c1743a17116",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.596445Z",
     "iopub.status.busy": "2025-12-28T07:34:59.596179Z",
     "iopub.status.idle": "2025-12-28T07:34:59.611996Z",
     "shell.execute_reply": "2025-12-28T07:34:59.611247Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.596423Z"
    },
    "id": "EVlEK1Yxl_AY",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# nn.MultiheadAttention can be used instead but for learning purposes, I built one from scratch\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, drop_out=0.1):\n",
    "    \"\"\"\n",
    "      d_model: the dimension of the embedding vector for EACH token\n",
    "      num_heads: the number of attention heads\n",
    "      drop_out: the dropout rate\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    assert d_model % num_heads == 0, \"MultiHeadAttention.__init__(): d_model must be divisible by num_heads\"\n",
    "    self.d_model   = d_model\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim  = d_model // num_heads\n",
    "\n",
    "    self.q_linear = nn.Linear(d_model, d_model)\n",
    "    self.v_linear = nn.Linear(d_model, d_model)\n",
    "    self.k_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # Final linear projection after concatenating heads (Learns how to merge all heads back into a single representation)\n",
    "    self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "  def forward(self, q, k, v, mask=None, cache=None):\n",
    "    '''\n",
    "     `cache` should be a dict of \n",
    "     {\n",
    "      \"K\": Tensor [B, num_heads, L_cached, d_head],\n",
    "      \"V\": Tensor [B, num_heads, L_cached, d_head],\n",
    "     }\n",
    "    '''\n",
    "    # x: [batch_size, seq_len, d_model] or [B, L, D]\n",
    "    B, L, D = q.shape\n",
    "\n",
    "    # Linear projections (trainable parameters) [B, L, D]\n",
    "    Q = self.q_linear(q)\n",
    "    K = self.k_linear(k)\n",
    "    V = self.v_linear(v)\n",
    "\n",
    "    # Split into heads: [B, L, num_heads, head_dim] → transpose → [B, num_heads, L, head_dim]\n",
    "    Q = Q.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    K = K.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    V = V.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    if cache is not None:\n",
    "        K = torch.cat([cache['K'], K], dim=2)\n",
    "        V = torch.cat([cache['V'], V], dim=2)\n",
    "    new_cache = {'K': K, 'V': V}\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    # Q      = [B, num_heads, L,        head_dim]\n",
    "    # K_T    = [B, num_heads, head_dim, L]\n",
    "    # scores = [B, num_heads, L,        L]\n",
    "    scale  = 1 / math.sqrt(self.head_dim) # prevents the softmax function from saturation -> extremely small gradients\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "\n",
    "    # Preventing the decoder from attending to future tokens (causal mask)\n",
    "    # Ignoring <pad> tokens\n",
    "    if mask is not None:\n",
    "      scores = scores.masked_fill(mask, float('-inf')) # masked_fill(mask, value) applies 'value' where 'mask == True'\n",
    "\n",
    "    attn = torch.softmax(scores, dim=-1) # [B, num_heads, L, L]\n",
    "    attn = self.dropout(attn)\n",
    "\n",
    "    output = torch.matmul(attn, V) # [B, num_heads, L, head_dim]\n",
    "\n",
    "    # Concatenate heads\n",
    "    output = output.transpose(1, 2).contiguous().view(B, L, D)  # [B, L, D]\n",
    "    output = self.out_proj(output)\n",
    "    assert output.shape == (B, L, D)\n",
    "\n",
    "    return output, new_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "08058af3-1d35-4bd0-9fd1-fa22b3bc5fd7",
    "_uuid": "34498fff-007b-454d-8568-847c1b144e7c",
    "collapsed": false,
    "id": "9bmOjM8Ib3aN",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "f8e2b8fb-e408-4a48-9793-7b91adf294c4",
    "_uuid": "fb7cffce-d6f1-4740-87e0-f0dc3d3d18df",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.613001Z",
     "iopub.status.busy": "2025-12-28T07:34:59.612705Z",
     "iopub.status.idle": "2025-12-28T07:34:59.631384Z",
     "shell.execute_reply": "2025-12-28T07:34:59.630571Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.612980Z"
    },
    "id": "txUfcRGHb25B",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, ffn_dim, drop_out):\n",
    "    super().__init__()\n",
    "    # self.attn = MultiHeadAttention(d_model, num_heads, is_enc=True) # torch version\n",
    "    self.attn = MultiHeadAttention(d_model, num_heads) # Minh's\n",
    "    self.ffn  = FeedForward(d_model, ffn_dim, drop_out)\n",
    "\n",
    "    # Normalization: https://medium.com/@florian_algo/batchnorm-and-layernorm-2637f46a998b\n",
    "    # Use normalization to prevent gradient vanishing \n",
    "    # -> ensuring that the feature values fall within the range where the activation function \n",
    "    #    is more sensitive to inputs, thereby avoiding gradient vanishing and speeding up convergence.\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout1 = nn.Dropout(drop_out)\n",
    "    self.dropout2 = nn.Dropout(drop_out)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    # 1. Attention\n",
    "    x2 = self.norm1(x)\n",
    "    attn1_output, _ = self.attn(x2, x2, x2, mask) # minh's multi-head attn\n",
    "    x = x + self.dropout1(attn1_output) \n",
    "    # x = x + self.dropout1(self.attn(x2, x2, x2, padding_mask=mask)) # torch\n",
    "\n",
    "    # 2. Feed-forward\n",
    "    x2 = self.norm2(x)\n",
    "    x = x + self.dropout2(self.ffn(x2))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "5bd4594d-f935-45f3-be9d-19cffdad57b7",
    "_uuid": "526ef6a5-aec4-4981-8cde-836cebbb9ffc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.632533Z",
     "iopub.status.busy": "2025-12-28T07:34:59.632251Z",
     "iopub.status.idle": "2025-12-28T07:34:59.650204Z",
     "shell.execute_reply": "2025-12-28T07:34:59.649480Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.632509Z"
    },
    "id": "w20sdjxcbLcA",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               d_model=512,\n",
    "               num_layers=6,\n",
    "               num_attn_heads=8,\n",
    "               ffn_dim=2048,\n",
    "               max_seq_len=64,\n",
    "               dropout=0.1):\n",
    "    \"\"\"\n",
    "      vocab_size: the size of the vocabulary\n",
    "      d_model: the dimension of the embedding vector for EACH token\n",
    "      num_layers: the number of encoder layers\n",
    "      num_attn_heads: the number of attention heads\n",
    "      ffn_dim: the dimension of the feed-forward network\n",
    "      max_seq_len: the maximum sequence length\n",
    "      dropout: the dropout rate\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "\n",
    "    self.embedder = Embedder(vocab_size, d_model)\n",
    "    self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "    self.encoder_layers = nn.ModuleList([\n",
    "        EncoderLayer(d_model, num_attn_heads, ffn_dim, dropout)\n",
    "        for _ in range(num_layers)\n",
    "    ])\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "  def forward(self, input_ids):\n",
    "    \"\"\"\n",
    "      input_ids: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    src_mask = create_pad_mask(input_ids)\n",
    "\n",
    "    # 1. Token Embedding\n",
    "    x = self.embedder(input_ids) * math.sqrt(self.d_model)\n",
    "    assert x.shape == (batch_size, seq_len, self.d_model), f\"[TransformerEncoder.forward()] - Token Embedding: Expect ({batch_size}, {seq_len}, {self.d_model}), got {x.shape}\"\n",
    "\n",
    "    # 2. Positional Encoding\n",
    "    x = self.positional_encoding(x)\n",
    "    x = self.dropout(x) # Use dropout to prevent overfitting\n",
    "    assert x.shape == (batch_size, seq_len, self.d_model), f\"[TransformerEncoder.forward()] - Pos Encoding: Expect ({batch_size}, {seq_len}, {self.d_model}), got {x.shape}\"\n",
    "\n",
    "    # 3. Encoder Layers\n",
    "    for encoder_layer in self.encoder_layers:\n",
    "      x = encoder_layer(x, src_mask)\n",
    "    assert x.shape == (batch_size, seq_len, self.d_model), f\"[TransformerEncoder.forward()] - Encoder Layers: Expect ({batch_size}, {seq_len}, {self.d_model}), got {x.shape}\"\n",
    "\n",
    "    return self.norm(x) # [batch size, source length, hidden dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2b6b6671-d28b-43c1-9f95-af93c645b7d5",
    "_uuid": "4b4d9593-c037-47ed-80f4-9213738a0301",
    "collapsed": false,
    "id": "Ey1DS8keb7AY",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "7179b0d1-25e0-4cf4-8e36-a8002abd3a06",
    "_uuid": "39fee791-670f-4b37-9e8f-cfb7e137b3de",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.651193Z",
     "iopub.status.busy": "2025-12-28T07:34:59.650948Z",
     "iopub.status.idle": "2025-12-28T07:34:59.669752Z",
     "shell.execute_reply": "2025-12-28T07:34:59.669087Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.651176Z"
    },
    "id": "nYeeT348lVto",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, ffn_dim, drop_out):\n",
    "    super().__init__()\n",
    "    self.attn1 = MultiHeadAttention(d_model, num_heads) # Minh's\n",
    "    self.attn2 = MultiHeadAttention(d_model, num_heads) # Minh's\n",
    "\n",
    "    self.ffn  = FeedForward(d_model, ffn_dim, drop_out)\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    self.dropout1 = nn.Dropout(drop_out)\n",
    "    self.dropout2 = nn.Dropout(drop_out)\n",
    "    self.dropout3 = nn.Dropout(drop_out)\n",
    "\n",
    "  def forward(self, x, enc_output, dec_masks, cache):\n",
    "    \"\"\"\n",
    "    x: [batch size, target length, hidden dim]\n",
    "    enc_output: [batch size, source length, hidden dim]\n",
    "    dec_mask: (enc_dec_mask [B, 1, 1, L], combined_mask [1, 1, L, L])\n",
    "    Note:\n",
    "    - In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
    "      and the memory keys and values come from the output of the encoder (https://arxiv.org/abs/1706.03762)\n",
    "    \"\"\"\n",
    "    enc_dec_mask    = dec_masks[0]\n",
    "    dec_pad_mask    = dec_masks[1]\n",
    "    dec_causal_mask = dec_masks[2]\n",
    "\n",
    "    # 1. Causally-masked Attention\n",
    "    self_attn_cache = cache if cache != None else None\n",
    "    x2 = self.norm1(x)\n",
    "    attn1_output, attn1_cache = self.attn1(x2, x2, x2, dec_pad_mask | dec_causal_mask, self_attn_cache) # Only cache causal attention\n",
    "    x = x + self.dropout1(attn1_output) # Minh's\n",
    "\n",
    "    # 2. Encoder-Decoder Attention\n",
    "    x2 = self.norm2(x)\n",
    "    attn2_output, _ = self.attn2(x2, enc_output, enc_output, enc_dec_mask) # enc_output = k = v -> same for every decoding step -> no caching\n",
    "    x = x + self.dropout2(attn2_output) # Minh's\n",
    "\n",
    "    # 3. Feed-forward\n",
    "    x2 = self.norm3(x)\n",
    "    x = x + self.dropout3(self.ffn(x2))\n",
    "\n",
    "    return x, attn1_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "daf758bc-ef32-4478-9b84-bfe4cf593190",
    "_uuid": "ec167f00-a6b4-4613-8c51-db9ee35ea76c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.670846Z",
     "iopub.status.busy": "2025-12-28T07:34:59.670651Z",
     "iopub.status.idle": "2025-12-28T07:34:59.688243Z",
     "shell.execute_reply": "2025-12-28T07:34:59.687456Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.670832Z"
    },
    "id": "oPWfcHb4Yrrj",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               d_model=512,\n",
    "               num_layers=6,\n",
    "               num_attn_heads=8,\n",
    "               ffn_dim=2048,\n",
    "               max_seq_len=64,\n",
    "               dropout=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.vocab_size = vocab_size\n",
    "\n",
    "    self.embedder = Embedder(vocab_size, d_model)\n",
    "    self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "    self.decoder_layers = nn.ModuleList([\n",
    "        DecoderLayer(d_model, num_attn_heads, ffn_dim, dropout)\n",
    "        for _ in range(num_layers)\n",
    "    ])\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "  def forward(self, src_ids, target_ids, encoder_outputs, cache=None):\n",
    "    batch_size, seq_len = target_ids.shape\n",
    "    dec_masks      = create_decoder_mask(src_ids, target_ids)\n",
    "\n",
    "    # 1. Token Embedding\n",
    "    x = self.embedder(target_ids) * math.sqrt(self.d_model)\n",
    "    assert x.shape == (batch_size, seq_len, self.d_model), f\"[TransformerDecoder.forward()] - Token Embedding: Expect ({batch_size}, {seq_len}, {self.d_model}), got {x.shape}\"\n",
    "\n",
    "    # 2. Positional Encoding\n",
    "    x = self.positional_encoding(x)\n",
    "    x = self.dropout(x) # Use dropout to prevent overfitting\n",
    "    assert x.shape == (batch_size, seq_len, self.d_model), f\"[TransformerDecoder.forward()] - Pos Encoding: Expect ({batch_size}, {seq_len}, {self.d_model}), got {x.shape}\"\n",
    "\n",
    "    # 3. Decoder Layers\n",
    "    new_caches = []\n",
    "    for layer_idx, decoder_layer in enumerate(self.decoder_layers):\n",
    "      layer_cache = None if cache == None else cache[layer_idx]\n",
    "      x, new_layer_cache = decoder_layer(x, encoder_outputs, dec_masks, layer_cache)\n",
    "      new_caches.append(new_layer_cache)\n",
    "    assert x.shape == (batch_size, seq_len, self.d_model), f\"[TransformerDecoder.forward()] - Encoder Layers: Expect ({batch_size}, {seq_len}, {self.d_model}), got {x.shape}\"\n",
    "\n",
    "    output = self.norm(x)\n",
    "    output = torch.matmul(output, self.embedder.embed.weight.transpose(0, 1)) # output: [batch_size, seq_len, vocab_size]\n",
    "    assert output.shape == (batch_size, seq_len, self.vocab_size)\n",
    "\n",
    "    return output, new_caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4939bd0-0f66-4f19-8526-7bcc75b6ef69",
    "_uuid": "6939a246-09e4-48e4-9754-a9edb7ac4374",
    "collapsed": false,
    "id": "a9yLnrLmliTB",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "41de2add-18ad-486c-a2ed-e66feead4e2b",
    "_uuid": "f711468b-9269-4a4d-ac31-8bb303520b1f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.689277Z",
     "iopub.status.busy": "2025-12-28T07:34:59.688987Z",
     "iopub.status.idle": "2025-12-28T07:34:59.708273Z",
     "shell.execute_reply": "2025-12-28T07:34:59.707571Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.689250Z"
    },
    "id": "lM48LVynwxLS",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self,\n",
    "               src_vocab_size,\n",
    "               trg_vocab_size,\n",
    "               max_seq_len=64,\n",
    "               num_layers=6,\n",
    "               num_attn_heads=8,\n",
    "               dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.src_vocab_size = src_vocab_size\n",
    "    self.trg_vocab_size = trg_vocab_size\n",
    "    self.encoder = TransformerEncoder(src_vocab_size, num_layers=num_layers, num_attn_heads=num_attn_heads, dropout=dropout)\n",
    "    self.decoder = TransformerDecoder(trg_vocab_size, num_layers=num_layers, num_attn_heads=num_attn_heads, dropout=dropout)\n",
    "    \n",
    "  def encode(self, src):\n",
    "      return self.encoder(src) # [batch size, source length, hidden dim]\n",
    "\n",
    "  def decode(self, src, trg, enc_output, cache=None):\n",
    "      return self.decoder(src, trg, enc_output, cache) # [batch_size, target_seq_len, trg_vocab_size]\n",
    "      \n",
    "  def forward(self, src, trg):\n",
    "    assert src.device == trg.device\n",
    "    enc_output = self.encode(src) \n",
    "    dec_output, _ = self.decode(src, trg, enc_output) \n",
    "\n",
    "    return dec_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2f62f86b-72f8-40e6-835d-63811cf53662",
    "_uuid": "a547aa0d-a4f7-441d-9039-e37d2572c32d",
    "collapsed": false,
    "id": "7p-b_lseWfCK",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "7bbc2500-215a-449e-8366-9ad4ae34f95b",
    "_uuid": "5b5c917b-37b9-48b5-8cd2-e09bd7766dd9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.709302Z",
     "iopub.status.busy": "2025-12-28T07:34:59.709051Z",
     "iopub.status.idle": "2025-12-28T07:34:59.727671Z",
     "shell.execute_reply": "2025-12-28T07:34:59.726832Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.709286Z"
    },
    "id": "l1gqTa2TWeec",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_seq_len = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_seq_len))\n",
    "    return res\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def l2_regularization(model, l2_lambda=1.0):\n",
    "    l2_norm = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name and 'norm' not in name:  # skip biases and LayerNorms\n",
    "            l2_norm += torch.sum(param.pow(2))\n",
    "    return l2_lambda * l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.728505Z",
     "iopub.status.busy": "2025-12-28T07:34:59.728330Z",
     "iopub.status.idle": "2025-12-28T07:34:59.745890Z",
     "shell.execute_reply": "2025-12-28T07:34:59.745258Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.728492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def log_to_csv(file_name, epoch, train_loss, val_loss, bleu_score):\n",
    "    with open(file_name, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch, train_loss, val_loss, bleu_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.746881Z",
     "iopub.status.busy": "2025-12-28T07:34:59.746630Z",
     "iopub.status.idle": "2025-12-28T07:34:59.768391Z",
     "shell.execute_reply": "2025-12-28T07:34:59.767707Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.746854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def greedy_batch(\n",
    "    model,\n",
    "    src,\n",
    "    max_seq_len,\n",
    "    trg_tokenizer\n",
    "):\n",
    "    trg_output = []\n",
    "    batch_len, _ = src.shape\n",
    "    for sentence_idx in range(batch_len):\n",
    "        src_sentence = src[sentence_idx, :].unsqueeze(0)\n",
    "        out_sentence = greedy_search(model, src_sentence, max_seq_len, trg_tokenizer)\n",
    "        trg_output.append(out_sentence)\n",
    "        \n",
    "    return trg_output\n",
    "    \n",
    "def greedy_search(\n",
    "    model,\n",
    "    src,\n",
    "    max_seq_len,\n",
    "    trg_tokenizer\n",
    ") -> str:\n",
    "    # 1. Create a translation tensor, init the first token to <bos>/<sos> (beginning/start of seq)\n",
    "    y_trg = torch.tensor([[trg_tokenizer.bos_id()]], device=src.device) # Current size [1, 1], will be [1, max_seq_len] eventually\n",
    "    enc_output = model.encode(src) # [batch size, source length, hidden dim]\n",
    "\n",
    "    last_cache = None\n",
    "    for _ in range(max_seq_len):\n",
    "        dec_output, _ = model.decode(src, y_trg, enc_output) # [1, trg_seq_len, trg_vocab_size]\n",
    "        # dec_output, last_cache = model.decode(src, y_trg[:, -1:], enc_output, last_cache) # [1, trg_seq_len, trg_vocab_size]\n",
    "        \n",
    "        # Get the last predicted token (at position trg_seq_len - 1)\n",
    "        # Return the token that has the highest vocab score\n",
    "        next_token = dec_output[:, -1, :].argmax(-1) # Return the index of the token with highest value\n",
    "        y_trg = torch.cat((y_trg, next_token.unsqueeze(0)), dim=-1)\n",
    "\n",
    "        if next_token.item() == trg_tokenizer.eos_id(): # Early break if <eos> has been generated\n",
    "            break\n",
    "    return trg_tokenizer.decode(y_trg.squeeze().tolist())\n",
    "    \n",
    "def beam_batch(\n",
    "    model,\n",
    "    src,\n",
    "    trg_tokenizer,\n",
    "    beam_size=3,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    length_penalty=0.75\n",
    "):\n",
    "    trg_output = []\n",
    "    batch_len, _ = src.shape\n",
    "    for sentence_idx in range(batch_len):\n",
    "        src_sentence = src[sentence_idx, :].unsqueeze(0)\n",
    "        out_sentence = beam_search(model, src_sentence, trg_tokenizer, beam_size, max_seq_len, length_penalty)\n",
    "        trg_output.append(out_sentence)\n",
    "        \n",
    "    return trg_output\n",
    "    \n",
    "def beam_search(\n",
    "        model, \n",
    "        src, \n",
    "        trg_tokenizer, \n",
    "        beam_size=3, \n",
    "        max_len=MAX_SEQ_LEN, \n",
    "        length_penalty=0.75\n",
    "    ) -> list:\n",
    "    \"\"\"\n",
    "    Beam search decoding for Transformer, adapted from https://arxiv.org/pdf/1609.08144 (7. Decoder - p.12)\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model with `encode` and `decode` methods.\n",
    "        src: Source tensor [1, src_len]\n",
    "        src_mask: Source padding mask [1, 1, src_len]\n",
    "        tokenizer: Object with tokenizer.eos_id, tokenizer.bos_id\n",
    "        beam_size: Number of beams to keep\n",
    "        max_len: Maximum generation length\n",
    "        length_penalty: Penalty to favor longer sequences [0.6, 0.7]\n",
    "    \"\"\"\n",
    "    bos = trg_tokenizer.bos_id\n",
    "    eos = trg_tokenizer.eos_id\n",
    "    \n",
    "    # 1. Encode source sentence\n",
    "    enc_output = model.encode(src)\n",
    "    \n",
    "    # 2. Initialize beam\n",
    "    beams = [{\n",
    "        \"tokens\": torch.tensor([[bos]], device=device), # [1, 1] -> will be [1, seq_len]\n",
    "        \"log_prob\": 0.0,\n",
    "        \"cache\": None\n",
    "    }]\n",
    "    \n",
    "    completed = [] # List storing complete combinations (expected to be `beam_size` to end the decoding loop)\n",
    "\n",
    "    # 3. Decoding loop\n",
    "    new_caches = None\n",
    "    for step in range(max_len):\n",
    "        all_candidates = []\n",
    "\n",
    "        # Find the next (beam_size) tokens at (t) to the previous 5 beams\n",
    "        # From the current 5 \n",
    "        # AB, -> ABC\n",
    "        #     -> ABD\n",
    "        #     -> ABQ\n",
    "        #     -> ABY\n",
    "        #     -> ABT\n",
    "        # AC, -> AC ... * 5\n",
    "        # AD, -> AD ... * 5\n",
    "        # DE, -> ditto\n",
    "        # DQ, -> ditto\n",
    "        # -> A total of 25 new combinations by the end of the for beam loop\n",
    "        for beam in beams:\n",
    "            y_trg = beam[\"tokens\"]\n",
    "\n",
    "            # Stop expanding if the last generated item is EOS\n",
    "            if y_trg[0, -1].item() == eos:\n",
    "                completed.append(beam)\n",
    "                continue\n",
    "\n",
    "            # Decode next token probabilities\n",
    "            out, new_caches = model.decode(src, y_trg[:, -1:], enc_output, new_caches) # [1, trg_seq_len, trg_vocab_size]\n",
    "            logits = out[:, -1, :]   # last timestamp logits [1, trg_vocab_size]\n",
    "            log_probs = F.log_softmax(logits, dim=-1) # [1, trg_vocab_size]\n",
    "\n",
    "            # Get top-k next tokens\n",
    "            topk_log_probs, topk_ids = log_probs.topk(beam_size, dim=-1)\n",
    "\n",
    "            # Expand beam\n",
    "            for k in range(beam_size):\n",
    "                # For a set of tokens in (t-1) -> create 5 more combinations of that set with new token in (t)\n",
    "                candidate = {\n",
    "                    \"tokens\": torch.cat([beam[\"tokens\"], topk_ids[:, k].unsqueeze(0)], dim=1),\n",
    "                    \"log_prob\": beam[\"log_prob\"] + topk_log_probs[0, k].item(),\n",
    "                }\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        # 4. Keep top-k beams\n",
    "        # Sort beams from high -> low score (score defined by lambda function)\n",
    "        # Short sentences -> smaller denominator -> higher score magnitude (more negative) -> slightly penalized (avoid too short).\n",
    "        all_candidates = sorted(\n",
    "            all_candidates, \n",
    "            key=lambda x: x[\"log_prob\"] / ((5 + len(x[\"tokens\"])) ** length_penalty / (5 + 1) ** length_penalty), \n",
    "            reverse=True\n",
    "        )\n",
    "        beams = all_candidates[:beam_size] # Select the top-k combinations\n",
    "\n",
    "        # Stop if all beams completed -> break the decoding loop early\n",
    "        if len(completed) >= beam_size:\n",
    "            break\n",
    "    \n",
    "    # If none of the beam has been properly decoded (has <eos> token at the end)\n",
    "    # Assign the completed list to whatever has been decoded\n",
    "    if not completed: \n",
    "        completed = beams\n",
    "\n",
    "    # 5. Choose best result\n",
    "    completed = sorted(\n",
    "        completed,\n",
    "        key=lambda x: x[\"log_prob\"] / ((5 + len(x[\"tokens\"])) ** length_penalty / (5 + 1) ** length_penalty),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Choose the beam with highest score ([\"tokens\"][0] ignore the dimension 0 (which is 1) [1, seq_len])\n",
    "    best = completed[0][\"tokens\"][0].tolist() \n",
    "    \n",
    "    return trg_tokenizer.decode(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f322288-b78f-4c4d-9801-68f2d9d6d871",
    "_uuid": "c8fa3306-fc7b-492c-ba00-22669ebc0c7f",
    "collapsed": false,
    "id": "M-eVkeckOVrO",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "0059e6ee-25ac-42fa-a205-1c7e9fe76655",
    "_uuid": "fc753a59-145f-4028-9be6-16b860819b8b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.769599Z",
     "iopub.status.busy": "2025-12-28T07:34:59.769311Z",
     "iopub.status.idle": "2025-12-28T07:34:59.787070Z",
     "shell.execute_reply": "2025-12-28T07:34:59.786285Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.769575Z"
    },
    "id": "Yr4MYf3XOgUq",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_PRINT_FREQ = 200\n",
    "\n",
    "def train(model, optimizer, scheduler, train_loader, criterion, device, epoch_id):\n",
    "  \"\"\"\n",
    "    Run one epoch\n",
    "  \"\"\"\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  # accuracy_meter   = AverageMeter()\n",
    "  batch_time_meter = AverageMeter()\n",
    "  loss_meter       = AverageMeter()\n",
    "\n",
    "  for i, batch in enumerate(train_loader):\n",
    "    start_time = time.time()\n",
    "\n",
    "    src = batch['input_ids'].to(device)\n",
    "    trg = batch['labels'].to(device)\n",
    "\n",
    "    trg_input = trg[:, :-1] # For each sequence, include everything excluding\n",
    "                            # the last token <eos>\n",
    "    trg_target = trg[:, 1:] # For each sequence, exclude the <sos> token\n",
    "\n",
    "    # Predict\n",
    "    output = model(src, trg_input)\n",
    "    preds = output.reshape(-1, output.size(-1)) # [B * L, V]\n",
    "    trg_target = trg_target.reshape(-1)        # [B * L]\n",
    "\n",
    "    # Loss computation\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(preds, trg_target)\n",
    "    # loss += l2_regularization(model, L2_LAMBDA) # Add L2 regularization \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    loss_meter.update(loss.item(), src.size(0))\n",
    "    batch_time_meter.update(time.time() - start_time)\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    if i % TRAIN_PRINT_FREQ == 0:\n",
    "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "            'Time(s) {time.val:.3f} ({time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                epoch_id, i, len(train_loader), time=batch_time_meter,\n",
    "                loss=loss_meter))\n",
    "      batch_time_meter.reset()\n",
    "    \n",
    "  return loss_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "90090e6c-8f64-4e88-9cc4-eea2bd46a9be",
    "_uuid": "dc10567d-b346-4ab9-b09e-451cfb6c1e6d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.788117Z",
     "iopub.status.busy": "2025-12-28T07:34:59.787906Z",
     "iopub.status.idle": "2025-12-28T07:34:59.803335Z",
     "shell.execute_reply": "2025-12-28T07:34:59.802610Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.788101Z"
    },
    "id": "hWupEgqFjvZz",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VAL_PRINT_FREQ = 100\n",
    "NUM_BLEU_EVAL = 10 # Calculate BLEU on the last 10 batches\n",
    "\n",
    "def validate(model, val_loader, criterion, device, trg_tokenizer, translate=False):\n",
    "  \"\"\"\n",
    "    Run one epoch\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  accuracy_meter   = AverageMeter()\n",
    "  batch_time_meter = AverageMeter()\n",
    "  loss_meter       = AverageMeter()\n",
    "  num_batch = len(val_loader)\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(val_loader):\n",
    "      start_time = time.time()\n",
    "\n",
    "      src = batch['input_ids'].to(device)\n",
    "      trg = batch['labels'].to(device)\n",
    "\n",
    "      trg_input  = trg[:, :-1] # For each sequence, include everything excluding\n",
    "                              # the last token <eos>\n",
    "      trg_target = trg[:, 1:] # For each sequence, exclude the <sos> token\n",
    "\n",
    "      # Predict\n",
    "      output     = model(src, trg_input)\n",
    "      preds      = output.reshape(-1, output.size(-1)) # [B * L, V]\n",
    "      trg_target = trg_target.reshape(-1)              # [B * L]\n",
    "      loss       = criterion(preds, trg_target)\n",
    "      \n",
    "      loss_meter.update(loss.item(), src.size(0))\n",
    "      batch_time_meter.update(time.time() - start_time)\n",
    "\n",
    "      total_loss += loss.item()\n",
    "      if translate:\n",
    "          # Metrics\n",
    "          # Use greedy search and beam search\n",
    "          # https://medium.com/nlplanet/two-minutes-nlp-most-used-decoding-methods-for-language-models-9d44b2375612\n",
    "\n",
    "          # 1. Translate sentences\n",
    "          torch.cuda.synchronize()\n",
    "          start_search_t = time.time()\n",
    "          \n",
    "          trg_sentences = greedy_batch(model, src, MAX_SEQ_LEN, trg_tokenizer)\n",
    "\n",
    "          torch.cuda.synchronize()\n",
    "          end_search_t = time.time()\n",
    "          print(f\"Beam search took {end_search_t - start_search_t:.4f} s\")\n",
    "          \n",
    "          # 2. Compute BLEU score\n",
    "          # Decode tokens into sentences\n",
    "          ref_trg_sentences = []\n",
    "          trg_sentence_tokens = trg[:, 1:-1]\n",
    "          for sentence_idx in range(trg_sentence_tokens.shape[0]):\n",
    "              trg_sentence = trg_tokenizer.decode(trg_sentence_tokens[sentence_idx].squeeze().tolist())\n",
    "              ref_trg_sentences.append(trg_sentence)\n",
    "\n",
    "          bleu = sacrebleu.corpus_bleu(trg_sentences, [ref_trg_sentences])\n",
    "          bleu_score = float(bleu.score)\n",
    "          \n",
    "          accuracy_meter.update(bleu_score)\n",
    "          print(f'Test: [{i}/{num_batch}] BLEU: {bleu_score:.4f} SRC: {ref_trg_sentences[0]} -> TRG: {trg_sentences[0]}')\n",
    "          \n",
    "      if i % VAL_PRINT_FREQ == 0:\n",
    "        print('Test: [{0}/{1}]\\t'\n",
    "              'Time(s) {time.val:.3f} ({time.avg:.3f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                  i, num_batch, time=batch_time_meter,loss=loss_meter))\n",
    "        batch_time_meter.reset()\n",
    "  print('* Val Avg BLEU {top1.avg:.3f}'.format(top1=accuracy_meter))\n",
    "\n",
    "  return loss_meter.avg, accuracy_meter.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "381c9d0b-bbb8-493c-90fb-3c271d73a397",
    "_uuid": "76ee2a02-e30e-4ea2-923a-807d54c157e3",
    "collapsed": false,
    "id": "VnacDqusSwgi",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Main (Training Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "19684ec8-01eb-467a-b3c8-f8d1dc1ae2cd",
    "_uuid": "327dae1a-48c6-451e-b845-37a580f69414",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.804424Z",
     "iopub.status.busy": "2025-12-28T07:34:59.804112Z",
     "iopub.status.idle": "2025-12-28T07:34:59.821088Z",
     "shell.execute_reply": "2025-12-28T07:34:59.820421Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.804406Z"
    },
    "id": "QsCUZEbCOVAT",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "cefb63e6-7551-4672-abbb-cd4a6be03905",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_DIR):\n",
    "  os.makedirs(SAVE_DIR)\n",
    "LOG_FILE_PATH = os.path.join(SAVE_DIR, 'log.csv')\n",
    "\n",
    "# Create the log file with header if it doesn't exist\n",
    "if not os.path.exists(LOG_FILE_PATH):\n",
    "    with open(LOG_FILE_PATH, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['epoch', 'train_loss', 'val_loss', 'bleu_score'])\n",
    "\n",
    "# Define model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, MAX_SEQ_LEN, NUM_LAYERS, NUM_HEADS, DROPOUT_RATE)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer, learning rate scheduler & loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = Scheduler(optimizer, d_model=HIDDEN_DIM, scale_factor=SCHEDULER_FACTOR)\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=TRG_PAD_IDX,\n",
    "    label_smoothing=SMOOTHING_FACTOR # smoothing factor: target word gets 1 - factor, all other classes get factor / (vocab_length - 1)\n",
    "                                     # -> tells the model that 'the labelling is not 100% accurate, be skeptical'\n",
    "    )\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Load the trained model\n",
    "if TRAIN_MODEL_PATH is not None:\n",
    "  checkpoint = torch.load(TRAIN_MODEL_PATH)\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "patience_cnt = 0\n",
    "best_loss = float('inf')\n",
    "# Training loop\n",
    "for epoch_id in range(NUM_EPOCHS):\n",
    "  # 1. Train the model first\n",
    "  train_loss = train(model, optimizer, scheduler, train_loader, criterion, device, epoch_id)\n",
    "\n",
    "  # 2. Test the current model on validation set -> performance evaluation\n",
    "  val_loss, bleu_score = validate(model, val_loader, criterion, device, trg_tkn)\n",
    "\n",
    "  log_to_csv(LOG_FILE_PATH, epoch_id, train_loss, val_loss, bleu_score)  \n",
    "  print(f'*** EPOCH {epoch_id}: Train Loss: {train_loss}; Val Loss {val_loss}')\n",
    "  if val_loss < best_loss - DELTA:\n",
    "    best_loss = val_loss\n",
    "    patience_cnt = 0\n",
    "  else:\n",
    "    patience_cnt += 1\n",
    "    if patience_cnt == PATIENCE:\n",
    "        print(f'Early stopping at {epoch_id + 1}') # EARLY STOPPING\n",
    "        torch.save({\n",
    "            'epoch': epoch_id + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, os.path.join(SAVE_DIR, file_name))\n",
    "        break\n",
    "                \n",
    "  # New strategy save a model every 10 epochs (due to kaggle limited output storage)\n",
    "  if (epoch_id + 1) % 10 == 0:\n",
    "    file_name = f\"epoch_{epoch_id}.pth\"\n",
    "    print(f'Saving... {file_name}')\n",
    "    torch.save({\n",
    "        'epoch': epoch_id + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, os.path.join(SAVE_DIR, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.821998Z",
     "iopub.status.busy": "2025-12-28T07:34:59.821808Z",
     "iopub.status.idle": "2025-12-28T07:34:59.837353Z",
     "shell.execute_reply": "2025-12-28T07:34:59.836718Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.821983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TEST_MODEL_PATH  = '/kaggle/input/model1/pytorch/default/1/epoch_109.pth' # IMPORTANT: Change testing model's filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T07:34:59.841265Z",
     "iopub.status.busy": "2025-12-28T07:34:59.840442Z",
     "iopub.status.idle": "2025-12-28T07:34:59.851077Z",
     "shell.execute_reply": "2025-12-28T07:34:59.850364Z",
     "shell.execute_reply.started": "2025-12-28T07:34:59.841245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, MAX_SEQ_LEN, dropout=DROPOUT_RATE)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer, learning rate scheduler & loss function\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=TRG_PAD_IDX,\n",
    "    label_smoothing=SMOOTHING_FACTOR # smoothing factor: target word gets 1 - factor, all other classes get factor / (vocab_length - 1)\n",
    "                                     # -> tells the model that 'the labelling is not 100% accurate, be skeptical'\n",
    "    )\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "if TEST_MODEL_PATH is not None:\n",
    "  checkpoint = torch.load(TEST_MODEL_PATH)\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "val_loss, bleu_score = validate(model, val_loader, criterion, device, trg_tkn, True)\n",
    "print(f'LOSS: {val_loss}; BLEU: {bleu_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T07:36:28.990912Z",
     "iopub.status.busy": "2025-12-28T07:36:28.990635Z",
     "iopub.status.idle": "2025-12-28T07:36:29.904679Z",
     "shell.execute_reply": "2025-12-28T07:36:29.903779Z",
     "shell.execute_reply.started": "2025-12-28T07:36:28.990892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BLEU: 12.703318703865365) 내일은 집에 갈 예정이예요.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, MAX_SEQ_LEN, dropout=DROPOUT_RATE)\n",
    "model = model.to(device)\n",
    "if TEST_MODEL_PATH is not None:\n",
    "  checkpoint = torch.load(TEST_MODEL_PATH)\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "eng_sentence = \"I am going home tomorrow.\"\n",
    "kor_sentence = '저는 내일 집에 갑니다.'\n",
    "src = torch.tensor(src_tkn.encode(eng_sentence)).unsqueeze(0)\n",
    "src = src.to(device)\n",
    "trg_tokens = greedy_batch(model, src, MAX_SEQ_LEN, trg_tkn)\n",
    "# trg_sentences = beam_batch(model, src, src_tkn)\n",
    "\n",
    "# 2. Compute BLEU score\n",
    "bleu = sacrebleu.corpus_bleu(trg_tokens, [[kor_sentence]])\n",
    "bleu_score = float(bleu.score)\n",
    "\n",
    "# Decode tokens into sentences\n",
    "print(f'(BLEU: {bleu_score}) {trg_tokens[0]}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "alA0J0IAchI5",
    "EYJcjuivbvCI",
    "Fb6XcFk5HQhF",
    "9bmOjM8Ib3aN",
    "Ey1DS8keb7AY",
    "a9yLnrLmliTB",
    "7p-b_lseWfCK"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8568855,
     "sourceId": 13496054,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8794787,
     "sourceId": 13811951,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 492105,
     "modelInstanceId": 476185,
     "sourceId": 631792,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 538519,
     "modelInstanceId": 524479,
     "sourceId": 695763,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
